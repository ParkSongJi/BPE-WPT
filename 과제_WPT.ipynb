{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParkSongJi/BPE-WPT/blob/main/%EA%B3%BC%EC%A0%9C_WPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  \"WordPiece Tokenizer\""
      ],
      "metadata": {
        "id": "NtSvBsKP1VGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* wordpiece tokenizer는 위에 설명한 subword tokenizer의 종류 중 하나이다. subword tokenizer에서 대표적으로 사용되는 방법으로 BPE(Byte Pair Encoding) 방법이 있다.\n",
        "\n",
        "* 일반적으로 많이 사용하는 Sentencepiece의 경우 빈도수를 기반으로 BPE를 수행하며, Wordpiece의 경우 likelihood를 기반으로 BPE를 수행한 알고리즘이다.\n",
        "\n",
        "* BERT의 경우 Wordpiece를 이용한 tokenizer를 사용하였고, sentencepiece를 사용한 모델 또한 많다. 선택에 따라 필요한 tokenizer를 활용할 수 있다."
      ],
      "metadata": {
        "id": "kfEjhv9d-Dlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"This is the Hugging Face course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]"
      ],
      "metadata": {
        "id": "7YG1pdGtqq9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 토큰화에 사용할 토크나이저로 bert-base-cased 를 사용\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "raZ3Ud_8qsP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 입력받은 corpus 를 바탕으로 단어의 구성을 확인하는 과정"
      ],
      "metadata": {
        "id": "uFUK2PVaU3Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 분석하고자 하는 문장의 빈도수 확인\n",
        "# defaultdict는 디폴트 값을 가지는 딕셔너리를 생성함\n",
        "from collections import defaultdict\n",
        "\n",
        "# 임의의 딕셔너리를 int 형태로 기본값 0으로 하여 만듦\n",
        "word_freqs = defaultdict(int)\n",
        "\n",
        "# 기존의 데이터를 전부 소문자로 바꾸는 작업 // 특수문자 없애는 작업\n",
        "corpus = list(map(str.lower, corpus))\n",
        "corpus = [text.replace(',', '').replace('.', '') for text in corpus]\n",
        "\n",
        "for text in corpus:\n",
        "    # words_with_offsets 변수 안에 반복문으로 들어온 문장들의 단어들에게 인덱스를 부여\n",
        "    # [('This', (0, 4)), ('is', (5, 7)), ('the', (8, 11)), ('Hugging', (12, 19)), ('Face', (20, 24)), ('course', (25, 31)), ('.', (31, 32))]\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "\n",
        "    # 토큰화 한 변수를 가지고 와서 각 단어만 저장\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "\n",
        "    # 이후 사전에 만들어 놓은 word_freqs 변수에 저장\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "word_freqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eARC0sQEqtxK",
        "outputId": "297cf33f-350f-42c6-b2c8-63b9b5efd0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'this': 3,\n",
              "             'is': 2,\n",
              "             'the': 1,\n",
              "             'hugging': 1,\n",
              "             'face': 1,\n",
              "             'course': 1,\n",
              "             'chapter': 1,\n",
              "             'about': 1,\n",
              "             'tokenization': 1,\n",
              "             'section': 1,\n",
              "             'shows': 1,\n",
              "             'several': 1,\n",
              "             'tokenizer': 1,\n",
              "             'algorithms': 1,\n",
              "             'hopefully': 1,\n",
              "             'you': 1,\n",
              "             'will': 1,\n",
              "             'be': 1,\n",
              "             'able': 1,\n",
              "             'to': 1,\n",
              "             'understand': 1,\n",
              "             'how': 1,\n",
              "             'they': 1,\n",
              "             'are': 1,\n",
              "             'trained': 1,\n",
              "             'and': 1,\n",
              "             'generate': 1,\n",
              "             'tokens': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 입력받은 corpus 를 바탕으로 vocaburlary 를 만드는 과정"
      ],
      "metadata": {
        "id": "_efnECFYUlt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet = []\n",
        "\n",
        "# word_freqs 변수 안에 있는 단어들의 첫 글자를 추출하여 해당글자가 alphabet 안에 포함되어 있지 않다면 추가\n",
        "for word in word_freqs.keys():\n",
        "    if word[0] not in alphabet:\n",
        "        alphabet.append(word[0])\n",
        "\n",
        "    # 각 알파벳의 첫 글자를 제외한 나머지 글자들을 추출하여 ## 을 붙이고 여기서 alphabet 안에 포함되어 있지 않다면 추가\n",
        "    for letter in word[1:]:\n",
        "        if f\"##{letter}\" not in alphabet:\n",
        "            alphabet.append(f\"##{letter}\")\n",
        "\n",
        "alphabet.sort()\n",
        "alphabet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8cs2DczqzQW",
        "outputId": "f12ed2a7-c359-43fa-cd62-b9429dc0ef96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['##a',\n",
              " '##b',\n",
              " '##c',\n",
              " '##d',\n",
              " '##e',\n",
              " '##f',\n",
              " '##g',\n",
              " '##h',\n",
              " '##i',\n",
              " '##k',\n",
              " '##l',\n",
              " '##m',\n",
              " '##n',\n",
              " '##o',\n",
              " '##p',\n",
              " '##r',\n",
              " '##s',\n",
              " '##t',\n",
              " '##u',\n",
              " '##v',\n",
              " '##w',\n",
              " '##y',\n",
              " '##z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'w',\n",
              " 'y']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어가 존재하지 않거나 오류가 발생했을시 사용할 수 있는 특수 조건 삽입\n",
        "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
      ],
      "metadata": {
        "id": "ZkotjSYwq05x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위에서 만들어 놓은 단어 별 빈도수의 디셔너리 word_freqs 를 가지고 와서 첫 번째 알파벳를 제외하고 나머지 알파벳들에게 ## 을 붙임\n",
        "splits = {\n",
        "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
        "    for word in word_freqs.keys()\n",
        "}\n",
        "print(splits)"
      ],
      "metadata": {
        "id": "UAw7Yiosq3EM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c87427a-9fbc-4a99-f587-a39236c418f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'this': ['t', '##h', '##i', '##s'], 'is': ['i', '##s'], 'the': ['t', '##h', '##e'], 'hugging': ['h', '##u', '##g', '##g', '##i', '##n', '##g'], 'face': ['f', '##a', '##c', '##e'], 'course': ['c', '##o', '##u', '##r', '##s', '##e'], 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'], 'about': ['a', '##b', '##o', '##u', '##t'], 'tokenization': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##a', '##t', '##i', '##o', '##n'], 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'], 'shows': ['s', '##h', '##o', '##w', '##s'], 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'], 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'], 'algorithms': ['a', '##l', '##g', '##o', '##r', '##i', '##t', '##h', '##m', '##s'], 'hopefully': ['h', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'], 'you': ['y', '##o', '##u'], 'will': ['w', '##i', '##l', '##l'], 'be': ['b', '##e'], 'able': ['a', '##b', '##l', '##e'], 'to': ['t', '##o'], 'understand': ['u', '##n', '##d', '##e', '##r', '##s', '##t', '##a', '##n', '##d'], 'how': ['h', '##o', '##w'], 'they': ['t', '##h', '##e', '##y'], 'are': ['a', '##r', '##e'], 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'], 'and': ['a', '##n', '##d'], 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'], 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈도의 점수를 계산하는 함수\n",
        "def compute_pair_scores(x):\n",
        "    # 임의의 딕셔너리 2개를 만들어 놓음\n",
        "    letter_freqs = defaultdict(int) # 빈도수\n",
        "    pair_freqs = defaultdict(int)   # 문자 쌍\n",
        "\n",
        "    # 딕셔너리 안에있는 단어와 그 빈도수를 가지고 옴\n",
        "    for word, freq in word_freqs.items():\n",
        "\n",
        "        # 함수에 입력된 딕셔너리(x)에 해당 단어에 대한 분리 결과를 가져옴\n",
        "        split = x[word]\n",
        "\n",
        "        # 만약 단어의 길이가 1일 경우에는 단어의 빈도수를 만들어 놓은 딕셔너리에 추가\n",
        "        if len(split) == 1:\n",
        "            letter_freqs[split[0]] += freq\n",
        "            continue\n",
        "\n",
        "        # 단어의 길이 -1 의 병합결과가 나오는것을 고려\n",
        "        for i in range(len(split) - 1):\n",
        "\n",
        "            # 나오는 순서대로 병합과정을 pair 라는 변수에 넣음\n",
        "            pair = (split[i], split[i + 1])\n",
        "\n",
        "            # 위와 같이 알파벳별 빈도수를 만들어 놓은 딕셔너리에 추가\n",
        "            letter_freqs[split[i]] += freq\n",
        "\n",
        "            # 위에서 생성한 pair 변수를 만들어 놓은 딕셔너리에 추가\n",
        "            pair_freqs[pair] += freq\n",
        "        # 마지막 알파벳의 빈도수도 더해줌, 마지막 알파벳은 합칠게 없어서 빈도수 추가가\n",
        "        letter_freqs[split[-1]] += freq\n",
        "\n",
        "    # 이후 생성된 빈도수, 문자 쌍 딕셔너리를 바탕으로 병합별 빈도 비중 계산\n",
        "    scores = {\n",
        "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
        "        for pair, freq in pair_freqs.items()\n",
        "    }\n",
        "    return scores"
      ],
      "metadata": {
        "id": "T6hwOqv5q44d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이후 만들어진 함수가 잘 작동하는지 앞에 위치한 병합 5개만 뽑음\n",
        "# 함수에 단어별로 분석된 딕셔너리 split을 넣고 임의의 변수에 넣어줌\n",
        "pair_scores = compute_pair_scores(splits)\n",
        "\n",
        "# 스코어 값이 나온 변수를 인덱스 번호와 함께 반복문을 돌림\n",
        "# 이때 생성된 pair_scores 딕셔너리는 병합된 조합이 들어있는 형태\n",
        "for i, key in enumerate(pair_scores.keys()):\n",
        "    print(f\"{key}: {pair_scores[key]}\")\n",
        "    if i >= 5:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHTbVEj1q6V_",
        "outputId": "db368e81-b7f1-44ef-e623-1363304feef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('t', '##h'): 0.0625\n",
            "('##h', '##i'): 0.03409090909090909\n",
            "('##i', '##s'): 0.02727272727272727\n",
            "('i', '##s'): 0.1\n",
            "('##h', '##e'): 0.011904761904761904\n",
            "('h', '##u'): 0.06666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이제 위에서 생성한 병합들의 스코어 중 가장 값이 큰 병합을 찾음\n",
        "# 가장 큰 스코어를 가진 병합을 넣을 변수와 스코어 값을 넣을 변수를 생성\n",
        "best_pair = \"\"\n",
        "max_score = None\n",
        "\n",
        "# 이어있는 스코어 변수 안에 모든 병합을 순차적으로 삽입, 넘어서면 교체하는 방식\n",
        "for pair, score in pair_scores.items():\n",
        "    if max_score is None or max_score < score:\n",
        "        best_pair = pair\n",
        "        max_score = score\n",
        "\n",
        "print(best_pair, max_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHwbmt6Mq71y",
        "outputId": "cb71f5a7-5dd6-44f6-abfd-af0497cd087e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('a', '##b') 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 큰 값을 가진 병합인 ab를 추가\n",
        "vocab.append(\"ab\")"
      ],
      "metadata": {
        "id": "NA69FZiprFow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이제 추가한 병합이 가지는 알파벳 부분을 합치고 업데이트하는 과정\n",
        "def merge_pair(a, b, x):\n",
        "    # 단어별 빈도수를 가지고 있는 딕셔너리인 word_ferqs 의 단어들을 하나씩 딕셔너리(x)에 해당 단어에 대한 분리 결과를 가져옴\n",
        "    for word in word_freqs:\n",
        "        split = x[word]\n",
        "        # print(split) # ['t', '##h', '##i', '##s']\n",
        "        # 해당 단어의 길이가 1인경우 아무 작업없이 넘어감\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        # 해당 단어가 여러 부분 단어로 분리된 경우, split 리스트를 순회하면서 a와 b가 인접한 경우를 찾음\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            # 만약 인접할경우 뒤에 나오는 알파벳의 ## 을 제거하고 병합해줌\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
        "                # 이후 기존의 split 에서 합쳐진 부분을 마지한 업데이트 split을 생성\n",
        "                split = split[:i] + [merge] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        # 완성된 결과를 최신화\n",
        "        x[word] = split\n",
        "    return x"
      ],
      "metadata": {
        "id": "dAd8J7RdrG26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge 후 업데이트 된 부분 확인하는 작업\n",
        "splits = merge_pair(\"a\", \"##b\", splits)\n",
        "splits[\"about\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU5E34TcrISJ",
        "outputId": "98ab6b53-05d3-4a8a-9300-b7ad20e6765c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ab', '##o', '##u', '##t']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 목표로하는 vocab 크기를 설정 후 채워질때까지 반복\n",
        "vocab_size = 70\n",
        "while len(vocab) < vocab_size:\n",
        "    # 점수를 측정하는 함수에 splits를 넣고 반복\n",
        "    scores = compute_pair_scores(splits)\n",
        "    best_pair, max_score = \"\", None\n",
        "    for pair, score in scores.items():\n",
        "        if max_score is None or max_score < score:\n",
        "            best_pair = pair\n",
        "            max_score = score\n",
        "    # 이후 점수가 가장 큰 병합을 splits에 업데이트\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "\n",
        "    # 가장 큰 병햡을 vocab에 채우는 과정\n",
        "    new_token = (\n",
        "        # best_pair[0] : 병합할 앞의 알파벳, best_pair[1][2:] : 병합할 뒤의 알파벳(##삭제)\n",
        "        best_pair[0] + best_pair[1][2:]\n",
        "        if best_pair[1].startswith(\"##\")\n",
        "        else best_pair[0] + best_pair[1]\n",
        "    )\n",
        "    # 나온 토큰(병합 구조)를 단어장에 추가\n",
        "    vocab.append(new_token)"
      ],
      "metadata": {
        "id": "vPcD9RJJrJht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flUJhwjFrK8w",
        "outputId": "158106be-805e-4d75-b1d2-79d9fe7d25ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', 'a', 'b', 'c', 'f', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'fa', 'fac', '##ct', '##ful', '##full', '##fully', '##hm', '##thm', 'is', '##thms', '##pt', '##apt', '##hapt', 'chapt', '##za', '##zat', 'abl', '##izat', '##izati', '##cti', '##iz', '##ithms', 'wi', 'wil', 'will', 'al', '##al', 'alg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_word(x):\n",
        "    # 인코딩된 부분 단어를 저장할 리스트 초기화\n",
        "    tokens = []\n",
        "    # word의 길이가 0보다 큰 동안 반복\n",
        "    while len(x) > 0:\n",
        "        # i를 현재 word의 길이로 초기화\n",
        "        i = len(x)\n",
        "        # i가 0보다 크고, word의 처음부터 길이 i까지의 부분 단어가 단어장에 없을 때 반복\n",
        "        while i > 0 and x[:i] not in vocab:\n",
        "            # i를 감소시킴\n",
        "            i -= 1\n",
        "        # 만약 i가 0이면, 단어장에 없는 부분 단어라는 의미\n",
        "        if i == 0:\n",
        "            # \"[UNK]\" (unknown token)을 반환하고 함수 종료\n",
        "            return [\"[UNK]\"]\n",
        "        # 단어장에 있는 부분 단어를 tokens 리스트에 추가\n",
        "        tokens.append(x[:i])\n",
        "        # 인코딩한 부분 단어를 제외한 나머지 부분을 word로 설정\n",
        "        x = x[i:]\n",
        "        # word의 길이가 0보다 크면\n",
        "        if len(x) > 0:\n",
        "            # 나머지 부분 단어에 \"##\"를 추가하여 다음 인코딩 준비\n",
        "            x = f\"##{x}\"\n",
        "    # 모든 부분 단어를 인코딩한 tokens 리스트 반환\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "zw0_CpNDrMST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 완성된 과정 확인\n",
        "* 위에서 만든 함수와 과정들이 제대로 작동하는지 확인해 보겠습니다"
      ],
      "metadata": {
        "id": "f7L3DgMAUXcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode_word(\"Hugging\"))\n",
        "print(encode_word(\"hugging\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P_1SjVzrPg0",
        "outputId": "8406d77a-a332-414d-e6f0-c917a28ea33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[UNK]']\n",
            "['h', '##u', '##g', '##g', '##i', '##n', '##g']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    # 입력받은 문장을 리스트로 변환하는 과정\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "\n",
        "    # pre_tokenized_text에 대해 encode_word 함수를 사용하여 각 단어를 부분 단어로 인코딩\n",
        "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
        "    return sum(encoded_words, [])"
      ],
      "metadata": {
        "id": "O_Hn6lenrRD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(\"This is the Hugging Face course!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfLEBcLmrTQ6",
        "outputId": "d8779dfc-1e56-4bd9-ec9a-969b2c44d790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK]',\n",
              " 'is',\n",
              " 't',\n",
              " '##h',\n",
              " '##e',\n",
              " '[UNK]',\n",
              " '[UNK]',\n",
              " 'c',\n",
              " '##o',\n",
              " '##u',\n",
              " '##r',\n",
              " '##s',\n",
              " '##e',\n",
              " '[UNK]']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eL4hTYo0rUnq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}